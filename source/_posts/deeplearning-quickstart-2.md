---
title: 转载 - 零基础入门深度学习(2) - 线性单元和梯度下降
date: 2019-05-03 11:57:39
mathjax: true
tags: [机器学习, 深度学习入门]
categories: [机器学习, 转载]
---

![](http://upload-images.jianshu.io/upload_images/2256672-06627c71f0d8c0dc.jpg)

> 无论即将到来的是大数据时代还是人工智能时代，亦或是传统行业使用人工智能在云上处理大数据的时代，作为一个有理想有追求的程序员，
不懂深度学习（Deep Learning）这个超热的技术，会不会感觉马上就out了？现在救命稻草来了，《零基础入门深度学习》
系列文章旨在讲帮助爱编程的你从零基础达到入门级水平。零基础意味着你不需要太多的数学知识，只要会写程序就行了，没错，
这是专门为程序员写的文章。虽然文中会有很多公式你也许看不懂，但同时也会有更多的代码，程序员的你一定能看懂的
（我周围是一群狂热的Clean Code程序员，所以我写的代码也不会很差）。

## 往期回顾

在上一篇文章中，我们已经学会了编写一个简单的感知器，并用它来实现一个线性分类器。你应该还记得用来训练感知器的『感知器规则』。
然而，我们并没有关心这个规则是怎么得到的。本文通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，
比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就掌握了机器学习的基本套路。

## 线性单元是啥

<!-- more -->

感知器有一个问题，当面对的数据集不是 **线性可分** 的时候，『感知器规则』可能无法收敛，
这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个 **可导** 的 **线性函数** 来替代感知器的 **阶跃函数** ，
这种感知器就叫做 **线性单元** 。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。

为了简单起见，我们可以设置线性单元的激活函数 $f$ 为

$$
    f(x) = x
$$

这样的线性单元如下图所示

![](http://upload-images.jianshu.io/upload_images/2256672-f57602e423d739ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对比此前我们讲过的感知器

![](http://upload-images.jianshu.io/upload_images/2256672-801d65e79bfc3162.png)

这样替换了激活函数 $f$ 之后， **线性单元** 将返回一个 **实数值** 而不是 **0,1分类** 。因此线性单元用来解决 **回归** 问题而不是 **分类** 问题。

### 线性单元的模型

当我们说 **模型** 时，我们实际上在谈论根据输入 $x$ 预测输出 $y$ 的 **算法** 。比如， $x$ 可以是一个人的工作年限， $y$ 可以是他的月薪，我们可以用某种算法来根据一个人的工作年限来预测他的收入。比如：

$$
    y = h(x) = w * x + b
$$

函数 $h(x)$ 叫做 **假设** ，而 $w$ 、 $b$ 是它的 **参数** 。我们假设参数 $w = 1000$ ，参数 $b = 500$ ，如果一个人的工作年限是5年的话，我们的模型会预测他的月薪为

$$
    y = h(x) = 1000 * 5 + 500 = 5500(元)
$$

你也许会说，这个模型太不靠谱了。是这样的，因为我们考虑的因素太少了，仅仅包含了工作年限。如果考虑更多的因素，比如所处的行业、公司、职级等等，可能预测就会靠谱的多。我们把工作年限、行业、公司、职级这些信息，称之为 **特征** 。对于一个工作了5年，在IT行业，百度工作，职级T6这样的人，我们可以用这样的一个特征向量来表示他 $\mathrm{x} = (5, IT, 百度, T6)$ 。

既然输入 $\mathrm{x}$ 变成了一个具备四个特征的向量，相对应的，仅仅一个参数 $w$ 就不够用了，我们应该使用4个参数 $w_1, w_2, w_3, w_4$ ，每个特征对应一个。这样，我们的模型就变成

$$
    y = h(x) = w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 + b
$$

其中， $x_1$ 对应工作年限， $x_2$ 对应行业， $x_3$ 对应公司， $x_4$ 对应职级。

为了书写和计算方便，我们可以令 $w_0$ 等于 $b$ ，同时令 $w_0$ 对应于特征 $x_0$ 。由于 $x_0$ 其实并不存在，我们可以令它的值永远为1。也就是说

$$
    b = w_0 * x_0, 其中 x_0 = 1
$$

这样上面的式子就可以写成

$$
\begin{aligned}
    y &= h(x) = w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 + b &\pod{1} \\
    & = w_0 * x_0 + w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 &\pod{2}
\end{aligned}
$$

我们还可以把上式写成向量的形式

$$
    y = h(x) = \mathrm{w}^\mathrm{T} \mathrm{x} \tag{式 1}
$$

长成这种样子模型就叫做 **线性模型** ，因为输出 $y$ 就是输入特 $x_1, x_2, x_3, \dots$ 征的 **线性组合** 。

### 监督学习和无监督学习

接下来，我们需要关心的是这个模型如何训练，也就是参数 $w$ 取什么值最合适。

机器学习有一类学习方法叫做 **监督学习** ，它是说为了训练一个模型，我们要提供这样一堆训练样本：每个训练样本既包括输入特征 $\mathrm{x}$ ，也包括对应的输出 $y$ ( $y$ 也叫做 **标记** ， **label** )。也就是说，我们要找到很多人，我们既知道他们的特征(工作年限，行业...)，也知道他们的收入。我们用这样的样本去训练模型，让模型既看到我们提出的每个问题(输入特征 $\mathrm{x}$ )，也看到对应问题的答案(标记 $y$ )。当模型看到足够多的样本之后，它就能总结出其中的一些规律。然后，就可以预测那些它没看过的输入所对应的答案了。

另外一类学习方法叫做 **无监督学习** ，这种方法的训练样本中只有 $\mathrm{x}$ 而没有 $y$ 。模型可以总结出特征的一些规律，但是无法知道其对应的答案 $y$ 。

很多时候，既有 $\mathrm{x}$ 又有 $y$ 的训练样本是很少的，大部分样本都只有 $\mathrm{x}$ 。比如在语音到文本(STT)的识别任务中， $\mathrm{x}$ 是语音， $y$ 是这段语音对应的文本。我们很容易获取大量的语音录音，然而把语音一段一段切分好并 **标注** 上对应文字则是非常费力气的事情。这种情况下，为了弥补带标注样本的不足，我们可以用 **无监督学习方法** 先做一些 **聚类** ，让模型总结出哪些音节是相似的，然后再用少量的带标注的训练样本，告诉模型其中一些音节对应的文字。这样模型就可以把相似的音节都对应到相应文字上，完成模型的训练。

### 线性单元的目标函数

现在，让我们只考虑 **监督学习** 。

在监督学习下，对于一个样本，我们知道它的特征 $\mathrm{x}$ ，以及标记 $y$ 。同时，我们还可以根据模型 $h(x)$ 计算得到输出 $\bar{y}$ 。注意这里面我们用 $y$ 表示训练样本里面的 **标记** ，也就是实际值；用带上划线的 $\bar{y}$ 表示模型计算的出来的 **预测值** 。我们当然希望模型计算出来的 $\bar{y}$ 和 $y$ 越接近越好。

数学上有很多方法来表示 $\bar{y}$ 和 $y$ 接近程度，比如我们可以用 $\bar{y}$ 和 $y$ 的差的平方的来表示它们的接近程度

$$
e = \frac{1}{2}(y - \bar{y})^2
$$

我们把 $e$ 叫做 **单个样本的误差** 。至于为什么前面要乘 $\frac{1}{2}$ ，是为了后面计算方便。

训练数据中会有很多样本，比如 $N$ 个，我们可以用训练数据中 **所有样本** 的误差的 **和** ，来表示模型的误差 $E$ ，也就是

$$
E = e^{(1)} + e^{(2)} + e^{(3)} + \dots + e^{(n)}
$$

上式的 $e^{(1)}$ 表示第一个样本的误差， $e^{(2)}$ 表示第二个样本的误差......。

我们还可以把上面的式子写成和式的形式。使用和式，不光书写起来简单，逼格也跟着暴涨，一举两得。所以一定要写成下面这样

$$
\begin{aligned}
    E &= e^{(1)} + e^{(2)} + e^{(3)} + \dots + e^{(n)} &\pod{3} \\
    &= \sum \limits_{i=0}^{n} e^{(i)} &\pod{4} \\
    &= \frac{1}{2} \sum \limits_{i=0}^{n} (y^{(i)} - \bar{y}^{(i)})^2 \pod{式2} &\pod{5}
\end{aligned}
$$

其中

$$
\begin{aligned}
    \bar{y}^{(i)} &= h(x^{(i)}) &\pod{6} \\
    &=\mathrm{w}^T\mathrm{x}^{(i)} &\pod{7}
\end{aligned}
$$

(式2)中， $x^{(i)}$ 表示第 $i$ 个训练样本的 **特征** ， $y^{(i)}$ 表示第 $i$ 个样本的 **标记** ，我们也可以用元组 $(x^{(i)}, y^{(i)})$ 表示第 $i$ 个 **训练样本** 。 $\bar{y}^{(i)}$ 则是模型对第 $i$ 个样本的 **预测值** 。

我们当然希望对于一个训练数据集来说，误差最小越好，也就是(式2)的值越小越好。对于特定的训练数据集来说， $(x^{(i)}, y^{(i)})$ 的值都是已知的，所以(式2)其实是参数 $\mathrm{w}$ 的函数。

$$
\begin{aligned}
    E &= \frac{1}{2} \sum \limits_{i=0}^{n} (y^{(i)} - \bar{y}^{(i)})^2 &\pod{8} \\
    &= \frac{1}{2} \sum \limits_{i=0}^{n} (y^{(i)} - \mathrm{w}^T\mathrm{x}^{(i)})^2 &\pod{9}
\end{aligned}
$$

由此可见，模型的训练，实际上就是求取到合适的 $\mathrm{w}$ ，使(式2)取得最小值。这在数学上称作 **优化问题** ，而 $E(\mathrm{w})$ 就是我们优化的目标，称之为 **目标函数** 。

### 梯度下降优化算法

大学时我们学过怎样求函数的极值。函数 $y= f(x)$ 的极值点，就是它的导数 $f'(x) = 0$ 的那个点。因此我们可以通过解方程 $f'(x) = 0$ ，求得函数的极值点 $(x_0, y_0)$ 。

不过对于计算机来说，它可不会解方程。但是它可以凭借强大的计算能力，一步一步的去把函数的极值点『试』出来。如下图所示：

![](http://upload-images.jianshu.io/upload_images/2256672-46acc2c2d52fc366.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480)

首先，我们随便选择一个点开始，比如上图的 $x_0$ 点。接下来，每次迭代修改 $x$ 为 $x_1, x_2, x_3, \dots$ ，经过数次迭代后最终达到函数最小值点。

你可能要问了，为啥每次修改 $x$ 的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数 $y = f(x)$ 的 **梯度** 的 **相反方向** 来修改 $x$ 。什么是 **梯度** 呢？翻开大学高数课的课本，我们会发现 **梯度** 是一个向量，它指向 **函数值上升最快** 的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改 $x$ 的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。

按照上面的讨论，我们就可以写出梯度下降算法的公式

$$
x_{new} = x_{old} - \eta \nabla f(x)
$$

其中， $\nabla$ 是 **梯度算子** ， $\nabla f(x)$ 就是指的 $f(x) 梯度。 $\eta$ 是步长，也称作 **学习速率** 。

对于上一节列出的目标函数(式2)

$$
E(\mathrm{w}) = \frac{1}{2} \sum \limits_{i=1}^{n} (y^{i} - \bar{y}^{(i)})^2
$$

梯度下降算法可以写成

$$
\mathrm{w}_{new} = \mathrm{w}_{old} - \eta \nabla E(\mathrm{w})
$$

聪明的你应该能想到，如果要求目标函数的 **最大值** ，那么我们就应该用梯度上升算法，它的参数修改规则是

$$
\mathrm{w}_{new} = \mathrm{w}_{old} + \eta \nabla E(\mathrm{w})
$$

下面，请先做几次深呼吸，让你的大脑补充足够的新鲜的氧气，我们要来求取 $\nabla E(\mathrm{w})$ ，然后带入上式，就能得到线性单元的参数修改规则。

关于 $\nabla E(\mathrm{w})$ 的推导过程，我单独把它们放到一节中。您既可以选择慢慢看，也可以选择无视。在这里，您只需要知道，经过一大串推导，目标函数 $E(\mathrm{w})$ 的梯度是

$$
\nabla E(\mathrm{w}) = - \sum \limits_{i=1}^{n} (y^{(i)} - \bar{y}^{(i)}) \mathrm{x}^{(i)}
$$

因此，线性单元的参数修改规则最后是这个样子

$$
\mathrm{w}_{new} = \mathrm{w}_{old} + \eta \sum \limits_{i=1}^{n} (y^{(i)} - \bar{y}^{(i)}) \mathrm{x}^{(i)} \pod{式3}
$$

有了上面这个式子，我们就可以根据它来写出训练线性单元的代码了。

需要说明的是，如果每个样本有M个特征，则上式中的 $\mathrm{x}, \mathrm{w}$ 都是M+1维 **向量** (因为我们加上了一个恒为1的虚拟特征 $x_0$，参考前面的内容)，而 $y$ 是 **标量** 。用高逼格的数学符号表示，就是

$$
\mathrm{x}, \mathrm{w} \in \mathrm{\Re}^{M+1} \\
y \in \Re^1
$$

为了让您看明白说的是啥，我吐血写下下面这个解释(写这种公式可累可累了)。因为 $\mathrm{w}, \mathrm{x}$ 是M+1维 **列向量** ，所以(式3)可以写成

$$
\begin{bmatrix}
    w_0 \\ w_1 \\ w_2 \\ \dots \\ w_m
\end{bmatrix}_{new}
=
\begin{bmatrix}
    w_0 \\ w_1 \\ w_2 \\ \dots \\ w_m
\end{bmatrix}_{old}
+ \eta \sum \limits_{i=1}^{n} (y^{(i)} - \bar{y}^{(i)})
\begin{bmatrix}
    1 \\ x_{1}^{(i)} \\ x_{2}^{(i)} \\ \dots \\ x_{m}^{(i)}
\end{bmatrix}
$$

如果您还是没看明白，建议您也吐血再看一下大学时学过的《线性代数》吧。

### $\nabla E(\mathrm{w})$ 的推导